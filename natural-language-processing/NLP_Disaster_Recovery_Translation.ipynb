{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune T5 locally for machine translation on COVID-19 Health Service Announcements with Hugging Face\n",
    "\n",
    "[![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/natural-language-processing/NLP_Disaster_Recovery_Translation.ipynb)\n",
    "\n",
    "This notebook requires running SageMaker Lab on an [Amazon Elastic Compute Cloud (Amazon EC2)]( https://aws.amazon.com/ec2/) GPU-enabled instance like the **g4dn.xlarge**. If you are not using that right now, please restart your session and select **GPU** to speed up the training process to tens of minutes rather than hours.\n",
    "\n",
    "If more information on training a large-scale machine translation model, see [Use Hugging Face on Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html)! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0. Install all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "ipywidgets\n",
    "git+https://github.com/huggingface/transformers\n",
    "datasets\n",
    "sacrebleu\n",
    "torch\n",
    "sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "# make sure to restart your kernel to use the newly install packages\n",
    "# IPython.Application.instance().kernel.do_shutdown(True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Explore the available datasets on Translators without Borders \n",
    "\n",
    "Next, download a preferred language pair for training the translation model. This notebook chose English to Spanish, but you can select any two languages.\n",
    "\n",
    "- Overall site page: https://tico-19.github.io/\n",
    "\n",
    "- Page with all language pairs: https://tico-19.github.io/memories.html \n",
    "\n",
    "Scroll through all supported language pairs and pick your favorite. This notebook demonstrates English to Spanish (`en-to-es`)\n",
    "\n",
    "Copy the link to that pair, for `en-to-es` it looks like this:\n",
    "- https://tico-19.github.io/data/TM/all.en-es-LA.tmx.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_my_data = 'https://tico-19.github.io/data/TM/all.en-es-LA.tmx.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget {path_to_my_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file = path_to_my_data.split('/')[-1]\n",
    "print (local_file)\n",
    "filename = local_file.split('.zip')[0]\n",
    "print (filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip {local_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract data from `.tmx` file type \n",
    "Next, you can use this local function to extract data from the `.tmx` file type and format for local training with Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste the name of your file and language codes here\n",
    "source_code_1 = 'en'\n",
    "target_code_2 =  'es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tmx(filename, source_code_1, target_code_2):\n",
    "    '''\n",
    "    Takes a local TMX filename and codes for source and target languages. \n",
    "    Walks through your file, row by row, looking for tmx / html specific formatting.\n",
    "    If there's a regex match, will clean your string and add to a dictionary for downstream pandas formatting.\n",
    "    '''\n",
    "    \n",
    "    data = {source_code_1:[], target_code_2:[]}\n",
    "\n",
    "    with open(filename) as f:\n",
    "\n",
    "        for row in f.readlines():\n",
    "\n",
    "            if not row.endswith('</seg></tuv>\\n'):\n",
    "                continue\n",
    "\n",
    "            if row.startswith('<seg>'):\n",
    "\n",
    "                st_1 = row.strip()\n",
    "\n",
    "                st_1 = st_1.replace('<seg>', '')\n",
    "                st_1 = st_1.replace('</seg></tuv>', '')\n",
    "\n",
    "                data[source_code_1].append(st_1)\n",
    "\n",
    "            # when you use your own target code, remove the -LA string \n",
    "            if row.startswith('<tuv xml:lang=\"{}-LA\"><seg>'.format(target_code_2)):\n",
    "\n",
    "                st_2 = row.strip()\n",
    "                # when you use your own target code, remove the -LA string \n",
    "                st_2 = st_2.replace('<tuv xml:lang=\"{}-LA\"><seg>'.format(target_code_2), '')\n",
    "                st_2 = st_2.replace('</seg></tuv>', '')\n",
    "\n",
    "                data[target_code_2].append(st_2)\n",
    "                \n",
    "        return data\n",
    "\n",
    "data = parse_tmx(filename, source_code_1, target_code_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes sure you got actual pairs\n",
    "assert len(data[source_code_1]) == len(data[target_code_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient = 'columns')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk in case you need to restart your kernel later\n",
    "df.to_csv('language_pairs.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Format extracted data for machine translation with Hugging Face\n",
    "\n",
    "Hugging Face’s translation module is documented on [GitHub](https://github.com/huggingface/transformers/tree/master/examples/pytorch/translation), and their official documentation also includes additional details for [loading the data set](https://huggingface.co/docs/datasets/loading_datasets.html#json-files).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('language_pairs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The translation task only supports custom [JSONLINES formatted files](https://jsonlines.org/). Each line is a dictionary with a key \"translation\" and its value another dictionary whose keys are the language pair. For example:\n",
    "\n",
    "```json\n",
    "{ \"translation\": { \"en\": \"Others have dismissed him as a joke.\", \"ro\": \"Alții l-au numit o glumă.\" } }\n",
    "{ \"translation\": { \"en\": \"And some are holding out for an implosion.\", \"ro\": \"Iar alții așteaptă implozia.\" } }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    \n",
    "    obj = {\"translation\": {source_code_1: row[source_code_1], target_code_2: row[target_code_2]}} \n",
    "    objs.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "!mkdir data\n",
    "with open('data/train.json', 'w') as f:\n",
    "    for row in objs:\n",
    "        j = json.dumps(row, ensure_ascii = False)\n",
    "        f.write(j)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Finetune a machine translation model locally\n",
    "\n",
    "Fourth, download the raw Python file from Hugging Face to fine-tune your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/translation/run_translation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full hugging face Trainer API args available here\n",
    "# https://github.com/huggingface/transformers/blob/de635af3f1ef740aa32f53a91473269c6435e19e/src/transformers/training_args.py\n",
    "# T5 trainig args available here\n",
    "# https://huggingface.co/transformers/model_doc/t5.html#t5config\n",
    "!python run_translation.py \\\n",
    "    --model_name_or_path t5-small \\\n",
    "    --do_train \\\n",
    "    --source_lang en \\\n",
    "    --target_lang es \\\n",
    "    --source_prefix \"translate English to Spanish: \" \\\n",
    "    --train_file data/train.json \\\n",
    "    --output_dir output/tst-translation \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --save_strategy epoch \\\n",
    "    --num_train_epochs 3\n",
    "#     --do_eval \\\n",
    "#     --validation_file path_to_jsonlines_file \\\n",
    "#     --dataset_name cov-19 \\\n",
    "#     --dataset_config_name en-es \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls output/tst-translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Test your newly fine-tuned translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(pretrained_model_name_or_path = 'output/tst-translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# line to make sure your model supports local inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifth, let's test it! Remember that, in using the default settings of only three epochs, your translation is probably not going to be SOTA. For achieving state of the art, (SOTA), we recommend migrating to Amazon SageMaker to scale up and out. Scaling up means moving your code to more advanced compute types, such as the [Amazon EC2 p4-series](https://aws.amazon.com/ec2/instance-types/) or [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/). Scaling out means adding more compute resources by increasing from one to many instances. Harnessing the power of the AWS cloud enables training across enormous datasets, which can produce more accurate models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = ['about how long have these symptoms been going on?',\t\n",
    "'and all chest pain should be treated this way especially with your age\t',\n",
    "'and along with a fever\t',\n",
    "'and also needs to be checked your cholesterol blood pressure',\t\n",
    "'and are you having a fever now?\t',\n",
    "'and are you having any of the following symptoms with your chest pain',\t\n",
    "'and are you having a runny nose?',\t\n",
    "'and are you having this chest pain now?',\n",
    "'and besides do you have difficulty breathing',\n",
    "'and can you tell me what other symptoms are you having along with this?',\n",
    "'and does this pain move from your chest?',\n",
    "'and drink lots of fluids',\n",
    "'and how high has your fever been',\n",
    "'and i have a cough too',\n",
    "'and i have a little cold and a cough',\n",
    "'''and i'm really having some bad chest pain today''']\n",
    "\n",
    "task_prefix = \"translate English to Spanish: \"\n",
    "\n",
    "for i in input_sequences:\n",
    "    input_ids = tokenizer('''{} {}'''.format(task_prefix, i), return_tensors='pt').input_ids\n",
    "    outputs = model.generate(input_ids)\n",
    "    print(i, tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('my-tf-en-to-sp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czf my_model.tar.gz my-tf-en-to-sp"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": ".conda-default:Python",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
