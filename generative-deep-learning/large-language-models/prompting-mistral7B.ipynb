{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6602da5-3c8d-40dd-9211-a0009357d69a",
   "metadata": {},
   "source": [
    "# Experimenting Prompt Engineering - Chatbot\n",
    "\n",
    "In this notebook we will explore common prompt engineering techniques to get the most out of Large Language Models (LLMs). \n",
    "\n",
    "First you will experiment by loading a 7 billion parameter LLM within the notebook environment and throwing some prompts its way to see how differt prompt strategies can impact the results of the model. \n",
    "\n",
    "After trying different prompt engineering techniques, you will learn how to run a simple chatbot using what you learned. \n",
    "\n",
    "Finally, we'll review some pointers for how to put these concepts into practice by building on the sample code and swapping in different LLMs to create your own solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091fdae7-bb30-4e60-a2ec-01eaf20657e2",
   "metadata": {},
   "source": [
    "### Working Environment \n",
    "\n",
    "[![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/build-on-aws/generative-ai-prompt-engineering/blob/main/prompt-engineering-chatbot/prompt-engineering-chatbot.ipynb)\n",
    "\n",
    "\n",
    "This notebook has been designed, written and tested to run for free on [Amazon SageMaker Studio Lab](https://studiolab.sagemaker.aws/) with CPU.  Studio Lab is a free machine learning (ML) development environment that provides compute and storage (up to 15GB) at no cost with NO credit card required.\n",
    "\n",
    "You can sign up for Amazon SageMaker Studio Lab here: [https://studiolab.sagemaker.aws/]\n",
    "\n",
    "> Whatever environment you end up using, make sure you have at least 12 GB of disk space available to run this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8a19a-8522-4bcf-b2a5-54cfcad0aead",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "First, if needed, install `ctransformers` - a library based on `transformers` from [Hugging Face](https://huggingface.co/), a great open source set of libraries for working and experimenting with the underlying technology of generative AI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa1ac45-da8a-441e-83d6-65e8132bba2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install ctransformers>=0.2.24 --quiet\n",
    "pip install langchain --quiet\n",
    "pip install transformers[torch] --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8dc104e-d8e9-489b-ab21-dbcecbc3ecca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04e3ec-9978-45f5-9a00-e41ba50804f4",
   "metadata": {},
   "source": [
    "## Mistral-7B-Instruct-v0.1-GGUP Model\n",
    "\n",
    "The üêã [Mistral-7B-Instruct-v0.1-GGUP](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF) üêã model was fine-tuned on top of Mistral 7B using using a variety of publicly available conversation datasets. \n",
    "\n",
    "### Loading the model\n",
    "\n",
    "The following cell will load the pretrained model. Because the model itself is around 3.8 GB, so it can take a minute or so to load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d237a5b-0ee3-4b48-a426-4048b186a375",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d96d6a748647ab89def13d53631a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf942e3d5f44069a3f0e63f5f96294e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a31903-630d-4b12-bea0-a77a872b61ac",
   "metadata": {},
   "source": [
    "# How do large language models work?\n",
    "\n",
    "## Prompt engineering\n",
    "\n",
    "Remember that LLMs are trained to predict the next word when given a sequence of words. \n",
    "\n",
    "![LLM-concept](img/LLM-generation.png)\n",
    "\n",
    "The way that we prompt an LLM can make a big difference in how it responds and how useful its output is. This depends not only on they type of query, but also nuances of how individual models where trained. Let's start by asking the Mistral model to describe the concept of photosynthesis.\n",
    "\n",
    "## Let's try a basic prompt\n",
    "\n",
    "In the following cell, we provide the model three parameters:\n",
    "1. the prompt: \"Explain photosythesis\"\n",
    "2. max_new_tokens which is the maximum length in tokens for the model's response\n",
    "3. temperature which you can think of as a measure of how *creative* the model can be in its response\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note:</b> If you are running this on a cpu instance of Studio Lab keep in mind that the following cell may take a couple minutes to run.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d5a91d1-2198-4583-90b0-8841885b3e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, water, and carbon dioxide into glucose, oxygen, and other chemical compounds. It is an essential process for life on Earth because it provides the primary source of organic compounds and oxygen.\n",
      "\n",
      "The process of photosynthesis can be divided into two main stages: the light-dependent reactions and the light-independent reactions (also known as the Calvin cycle).\n",
      "\n",
      "During the light-dependent reactions, light energy is absorbed by pigments in the chloroplasts of plant cells, primarily chlorophyll. This energy is used to split water molecules into hydrogen ions and electrons. The hydrogen ions are used to create ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate), which are energy-rich molecules that are used in the second stage of photosynthesis. The electrons from water molecules are passed through a series of protein complexes in the thylakoid membrane of the chloroplasts,"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain photosythesis\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79335c8f-ede7-4a65-813e-ec225822d5db",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model did pretty well, but let's see if we can make the answer more concise</b></div>\n",
    "\n",
    "## Modifying the basic prompt\n",
    "Below we will try giving the model more specific instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18fe60fe-bc65-4855-819e-a7ecb3071e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, water, and carbon dioxide into glucose, oxygen, and other chemical compounds. This process occurs in the chloroplasts of plant cells and involves the absorption of light energy by pigments such as chlorophyll. The energy from the absorbed light is used to drive a series of chemical reactions that ultimately result in the production of glucose, which serves as a source of energy for the plant."
     ]
    }
   ],
   "source": [
    "prompt = \"Explain photosythesis in three sentences.\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7071987-b179-4cd2-900c-28e02894c7d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>This response is more concise; however, the model didn't answer in three sentences. We will explore an approach a little further below when we cover few-shot learning.</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc398e-65a6-4ed6-99bc-19a3f2fa0498",
   "metadata": {},
   "source": [
    "# Instruction tuning\n",
    "\n",
    "Building complex prompts to achieve simple objectives like this one can get complicated very fast, especially once we attempt more sophisticated tasks. Luckily, LLMs can be instruction tuned. This is done on using a variety of publicly available conversation datasets which fine-tune the model to follow directions as closely as possible. In other words, instead of providing examples in each prompt, this is done by changing the model weights as part of training processs. This Mistral model has actually already been instruction tuned and we just need to follow the format which was used during training. The model uses ChatML markup language with <|im_start|> and <|im_end|> tokens added:\n",
    "```\n",
    "<|im_start|>system\n",
    "{system_message}<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```\n",
    "\n",
    "Here is an example prompt exchange using the model template:\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "You are MistralInstruct, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "How are you?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "I am doing well!<|im_end|>\n",
    "<|im_start|>user\n",
    "Please tell me about how mistral winds have attracted super-orcas.<|im_end|>\n",
    "```\n",
    "\n",
    "## Let's try it:\n",
    "\n",
    "The following cell uses the prompt template to provide a system prompt to the model to ask it to provide its reasoning step-by-step which can help LLM response accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df8390b3-2d2b-4bfb-ab8c-223dc5455321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<|im_start|>system\n",
      "You are MistralInstruct, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Please tell me about how mistral winds have attracted super-orcas. Keep your answer brief.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "Response:\n",
      "Mistral winds are strong, cold winds that blow from the northwest towards the southeast in the North Atlantic Ocean. They are known for their strength and persistence, with speeds of up to 60 mph (96 km/h) and can last for days or even weeks.\n",
      "\n",
      "Super-orcas, also known as humpback whales, are large marine mammals that migrate long distances in search of food and breeding grounds. They are attracted to areas with abundant krill populations, which they feed on.\n",
      "\n",
      "Mistral winds have attracted super-orcas by providing a reliable source of nutrients for them. The cold water temperature of the North Atlantic Ocean is ideal for krill growth, and the strong mistral winds help to distribute the krill throughout the ocean, making it easier for the whales to find them. Additionally, the mistral winds can create upwelling events, where nutrient-rich water rises from the deep ocean to the surface, providing an even more abundant food source for the whales.\n",
      "\n",
      "Overall, the combination of strong mistral winds and abundant krill populations has made the North Atlantic Ocean a prime destination for super-orcas, attracting large numbers of these majestic creatures each year."
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"\n",
    "<|im_start|>system\n",
    "You are MistralInstruct, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Please tell me about how mistral winds have attracted super-orcas. Keep your answer brief.<|im_end|>\n",
    "<|im_start|>assistant\\n\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=500, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b7a08-1c97-47ef-a659-7cf30315567d",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model was able to follow the system direction to provide its step-by-step reasoning. However, manually supplying the full prompt template for each query is cumberson. In the next section we will look at how to simplify and automate prompt generation dynamically.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046f28b-a89f-41f5-9d54-89263a396039",
   "metadata": {},
   "source": [
    "# Dyanamic prompting\n",
    "Now that we know how to submit instructions to Mistral LLM, let's try assembling prompts dynamically - part of the prompt can be fixed, and part can be provided on the fly. We achieve this by creating prompt template with variables. The value for each variable is supplied in a separate statement and that statement can be executed further down in the code. Basically, this is a way to de-couple static part of the prompt from variable one, making the entire prompt dynamic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa310f2-356c-47aa-9679-cb7cba07ed48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"<|im_start|>system\n",
    "You are an AI assistant which gives helpful and polite answers to the user's questions.<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\\n\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59b97e69-cfaf-4d51-973c-797629f2c86b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You are an AI assistant which gives helpful and polite answers to the user's questions.<|im_end|>\n",
      "<|im_start|>user\n",
      "Explain photosynthesis in three sentences<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Response:\n",
      "Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, water, and carbon dioxide into glucose, oxygen, and other chemical compounds. It is a vital process for life on Earth as it provides the primary source of energy for most living organisms. The process can be simplified into two stages: the light-dependent reactions, which occur in the thylakoid membranes of chloroplasts, and the light-independent reactions, also known as the Calvin cycle, which take place in the stroma of chloroplasts."
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(question=\"Explain photosynthesis in three sentences\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=500, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8c068-56e8-46a1-b2e2-f4c01e115c95",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>Using prompt templates makes it much easier to automate away the formatting nuances of different LLMs which can vary widely.</b></div>\n",
    "\n",
    "\n",
    "# Few-shot prompts\n",
    "\n",
    "We can influence model's output style by using what's called few-shot prompting - a technique which shows the model the exact behavior we expect on few examples. Few-shot prompting can be a powerful approach to improve model performance for a given task without additional training or fine-tuning. \n",
    "\n",
    "As you can see in the cell below, we provide the model two examples of a prompt and the expected response where the number of response sentences is constrained correctly by the request. We end the full prompt with our request for a three sentence explanation of photosenthesis and let the model complete the example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49a23538-bed3-46c5-9343-16aaf9a86f72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>user\n",
      "Explain precipitation in two sentences.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "1. In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.\n",
      "2. The main forms of precipitation include drizzle, rain, sleet, snow, ice pellets, graupel and hail.<|im_end|>\n",
      "\n",
      "<|im_start|>user\n",
      "Explain condensation in one sentence.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "1. Condensation is the change of the state of matter from the gas phase into the liquid phase, and is the reverse of vaporization.<|im_end|>\n",
      "\n",
      "<|im_start|>user\n",
      "Explain photosynthesis in three sentences.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Response:\n",
      "1. Photosynthesis is the process by which plants, algae and some bacteria convert sunlight, water and carbon dioxide into oxygen, glucose and other organic compounds.\n",
      "2. It is a vital process for life on Earth as it provides the primary source of energy for almost all living organisms.\n",
      "3. The process can be divided into two stages: the light-dependent reactions, which occur in the thylakoid membranes of the chloroplasts, and the light-independent reactions, also known as the Calvin cycle, which take place in the stroma of the chloroplasts."
     ]
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "fs_template = PromptTemplate.from_template(\n",
    " \"\"\"<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{output}<|im_end|>\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"Explain precipitation in two sentences.\",\n",
    "    \"output\": \"1. In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.\\n2. The main forms of precipitation include drizzle, rain, sleet, snow, ice pellets, graupel and hail.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Explain condensation in one sentence.\",\n",
    "    \"output\": \"1. Condensation is the change of the state of matter from the gas phase into the liquid phase, and is the reverse of vaporization.\"\n",
    "  }\n",
    "]\n",
    "\n",
    "prompt_fs = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=fs_template,\n",
    "    suffix=\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_fs.format(question=\"Explain photosynthesis in three sentences.\")\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "      \n",
    "print(\"Response:\")      \n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4fb9a-cb8a-4940-9715-4d70f4c215b0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Using a couple examples, we were able to teach the model to respond by specifically numbering the sentences in its response and limiting its response to the number of sentences requested.</b></div>\n",
    "\n",
    "\n",
    "# Augmenting LLM response with context\n",
    "\n",
    "Without providing any real-time context, the model is limited to knowledge it gained during training which may be incomplete or out of date. However, we can improve responses by providing relevant context that the model can use before formulating a response. \n",
    "\n",
    "To illustrate the benefit of providing context we will first query the model about which instance types are available to use for managed spot training in SageMaker. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f102be3d-1d7f-43c9-aa79-dbc2ccd21753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You are an AI assistant which gives helpful and polite answers to the user's questions.<|im_end|>\n",
      "<|im_start|>user\n",
      "Which instances can I use with Managed Spot Training in SageMaker?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Response:\n",
      "You can use Managed Spot Training in Amazon SageMaker with the following instances:\n",
      "\n",
      "* ml.c4.xlarge\n",
      "* ml.c4.2xlarge\n",
      "* ml.c4.4xlarge\n",
      "* ml.c4.8xlarge\n",
      "* ml.p2.xlarge\n",
      "* ml.p2.8xlarge\n",
      "* ml.p3.xlarge\n",
      "* ml.p3.16xlarge\n",
      "* ml.m4.xlarge\n",
      "* ml.m4.2xlarge\n",
      "* ml.m4.4xlarge\n",
      "* ml.m4.10xlarge\n",
      "* ml.m4.16xlarge\n",
      "* ml.c5.xlarge\n",
      "* ml.c5.2xlarge\n",
      "* ml.c5.4xlarge\n",
      "* ml.c5.8xlarge\n",
      "* ml.c5.16xlarge\n",
      "* ml.p3.16xlarge\n",
      "* ml.p3.20xlarge\n",
      "* ml.p3.24xlarge\n",
      "* ml.p3.32xlarge\n",
      "\n",
      "Note that the availability of these instances may vary depending on your region and availability zone."
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(question=\"Which instances can I use with Managed Spot Training in SageMaker?\")\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")      \n",
    "for text in llm(prompt, max_new_tokens=300, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b4409-3c5e-4c23-8a3b-39897bfee683",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>This response does not clearly and directly answer our question and you can see how without provided context, the LLM uses its own \"knowledge\" to answer questions. Let's try providing some context to the model to use as reference. We will add a new dynamic field \"context\" to our prompt template and provide up to date information when we ask the user question.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6110e2e4-773f-4ff6-a0d3-3d33073cae42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You are an AI assistant which gives helpful, detailed, and polite answers to the user's questions<|im_end|>\n",
      "<|im_start|>user\n",
      "Given the context below, answer the question that follows. If you do not know the answer and the context doesn't\n",
      "contain the answer truthfully say \"I don't know\".\n",
      "\n",
      "Context: Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n",
      "Question: Which instances can I use with Managed Spot Training in SageMaker?<|im_end|>\n",
      "\n",
      "Response:\n",
      "<|im_start|>system\n",
      "You can use any instance supported in Amazon SageMaker with Managed Spot Training."
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "context_template = PromptTemplate.from_template(\n",
    "    \"\"\"<|im_start|>system\n",
    "You are an AI assistant which gives helpful, detailed, and polite answers to the user's questions<|im_end|>\n",
    "<|im_start|>user\n",
    "Given the context below, answer the question that follows. If you do not know the answer and the context doesn't\n",
    "contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}<|im_end|>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "context = \"\"\"Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\"\"\"\n",
    "\n",
    "\n",
    "prompt=context_template.format(context=context, question=\"Which instances can I use with Managed Spot Training in SageMaker?\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31be60b-e4e5-4e3c-98e2-a8ef42ceb889",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>Great! The model used the provided context to answer the user question. Now lets use the same context and ask the model about something that is not covered in the context. </b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f59ac84e-dd06-46c2-88ef-d5f0c9547f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You are an AI assistant which gives helpful, detailed, and polite answers to the user's questions<|im_end|>\n",
      "<|im_start|>user\n",
      "Given the context below, answer the question that follows. If you do not know the answer and the context doesn't\n",
      "contain the answer truthfully say \"I don't know\".\n",
      "\n",
      "Context: Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n",
      "Question: Which instances can I use with Amazon ECS?<|im_end|>\n",
      "\n",
      "Response:\n",
      "\n",
      "Answer: I don't know, Amazon ECS does not support managed spot training."
     ]
    }
   ],
   "source": [
    "prompt=context_template.format(context=context, question=\"Which instances can I use with Amazon ECS?\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4324af-bfa3-4c47-a43b-ee6393fad0c5",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model followed our direction and admitted that it did not know the answer when it wasn't able to be determined from the provided context. This is a powerful method that is used heavily in RAG (Retrieval Augmented Generation) applications. A company may only want an IT support bot to provide answers based on internal documentaiton provided to the model as context. </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd469ce-4e37-4cec-8168-cadd5d072237",
   "metadata": {},
   "source": [
    "# Hallucinations\n",
    "\n",
    "Hallucination is when a model makes up information and presents it as if it were true. Leading questions can often cause hallucinations. Let's see what happens if we ask about a non-existent new launch at re:Invent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51c3685e-03e7-4072-bc8c-8a67b63aaa7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You are an AI assistant which gives helpful and polite answers to the user's questions.<|im_end|>\n",
      "<|im_start|>user\n",
      "What type of new chemical solution AWS announced at re:Invent this year?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Response:\n",
      "AWS announced a new chemical solution called \"AWS Polymer\" at re:Invent this year. AWS Polymer is a cloud-based platform that enables users to design, simulate, and optimize complex chemical processes using machine learning algorithms. The platform can be used for drug discovery, materials science, and other applications in the chemical industry."
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(question=\"What type of new chemical solution AWS announced at re:Invent this year?\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa8272-6572-47da-a365-b23ac32be304",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model incorrectly states that AWS announced a new chemical solution called \"AWS Graviton2\" in 2019. One approach to combatting LLM hallucination is to give the model an \"out\" by giving it explicit instructions not to make up information and admit when it doesn't know something. Let's try the same question again by by also providing new system instructions in our prompt. </b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3666f316-d20c-4788-9c44-31b3035bd5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You are an AI assistant which answers user questions politely and in great details. Do not make up facts. Say I don't know if you have no information about something.<|im_end|>\n",
      "<|im_start|>user\n",
      "What type of new chemical solution AWS announced at reInvent this year?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Response:\n",
      "AWS announced a new chemical solution called \"AWS Elemental X\" at reInvent this year. AWS Elemental X is a high-performance, low-cost, and scalable solution that enables customers to run complex chemical simulations and modeling workloads on the cloud. The solution leverages AWS's advanced computing resources, including GPUs, FPGAs, and TPUs, as well as its deep expertise in molecular simulation and modeling. With AWS Elemental X, customers can accelerate their research and development efforts, reduce costs, and improve the accuracy of their simulations and models."
     ]
    }
   ],
   "source": [
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    \"\"\"<|im_start|>system\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\\n\"\"\"\n",
    ")\n",
    "\n",
    "system = \"You are an AI assistant which answers user questions politely and in great details. Do not make up facts. Say I don't know if you have no information about something.\"\n",
    "question = \"What type of new chemical solution AWS announced at reInvent this year?\"\n",
    "\n",
    "prompt = prompt_template2.format(system=system, question=question)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f21e2-4b5f-4313-86e5-3885b2f10f1d",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>By giving the model clear instructions and an out, we were able to get the model to admit it didn't have enough context to answer the question rather than making up something.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fb206-6661-4ba6-95c5-6369176ff869",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "\n",
    "Let's make a simple chatbot.  There is no special library to include and no setting to apply to the LLM, all we need is prompt engineering!\n",
    "\n",
    "Let's use what we know of prompts with in context learning, to create a simple chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "898d619d-6c08-46d4-a0ba-cb1a9e23a7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You are an AI assistant which answers user questions politely and in great details. Do not make up facts. Say I don't know if you have no information about something.<|im_end|>\n",
      "<|im_start|>user\n",
      "Who is Jeff Bezos?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Response:\n",
      "Jeff Bezos is an American entrepreneur and business magnate. He was born on January 28, 1964 in Seattle, Washington. He is best known as the founder and CEO of Amazon, one of the world's largest online retailers. He has also founded Blue Origin, a space exploration company, and The Washington Post, a daily newspaper. Bezos has been ranked as one of the richest people in the world by Forbes magazine.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is Jeff Bezos?\"\n",
    "\n",
    "prompt = prompt_template2.format(system=system, question=question)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "resp = llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"<|im_end|>\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75da0c-cce2-4f04-8cc0-65ea5b1efc5b",
   "metadata": {},
   "source": [
    "### Follow-up questions\n",
    "Now if we ask the LLM a follow-up question without providing context from the conversation so far, it will not be able to provide an accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae5b02ab-f7d3-4022-b26d-85284e8fe9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You are an AI assistant which answers user questions politely and in great details. Do not make up facts. Say I don't know if you have no information about something.<|im_end|>\n",
      "<|im_start|>user\n",
      "How old is he?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Response:\n",
      "I am sorry, but I do not have any information about who you are referring to. Could you please provide me with more details or context so that I can assist you better?"
     ]
    }
   ],
   "source": [
    "question = \"How old is he?\"\n",
    "\n",
    "prompt = prompt_template2.format(system=system, question=question)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=300, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9422bf01-56ab-4d94-8395-bc8ffb592dd6",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model doesn't know who \"he\" is. Without the chat history context it is not able to answer.</b></div>\n",
    "\n",
    "### Providing chat context\n",
    "Let's use the context template we created earlier and feed in the previous response about Jeff Bezos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4500db7-a2da-40ce-b525-a33eacb46c72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You are an AI assistant which gives helpful, detailed, and polite answers to the user's questions<|im_end|>\n",
      "<|im_start|>user\n",
      "Given the context below, answer the question that follows. If you do not know the answer and the context doesn't\n",
      "contain the answer truthfully say \"I don't know\".\n",
      "\n",
      "Context: Jeff Bezos is an American entrepreneur and business magnate. He was born on January 28, 1964 in Seattle, Washington. He is best known as the founder and CEO of Amazon, one of the world's largest online retailers. He has also founded Blue Origin, a space exploration company, and The Washington Post, a daily newspaper. Bezos has been ranked as one of the richest people in the world by Forbes magazine.\n",
      "Question: How old is he?<|im_end|>\n",
      "\n",
      "Response:\n",
      "\n",
      "Answer: Jeff Bezos was born on January 28, 1964, so he is currently 57 years old."
     ]
    }
   ],
   "source": [
    "prompt = context_template.format(context=resp, question=question)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=300, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84aba05-9ae9-4511-a506-8bbf558854e9",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>This is a simplified example, but by including the past chat context, we can support a back and forth chat bot interaction with the LLM.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd190db-6d79-4a00-9d67-b0bfb4019a3f",
   "metadata": {},
   "source": [
    "# More examples of LLM use cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7671b-87da-453b-a400-6f125d644b38",
   "metadata": {},
   "source": [
    "## The following are optional prompts you can try running to keep exploring.\n",
    "\n",
    "\n",
    "For simpler questions/prompts we can often get away witout strict instruction formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c507742b-dab9-4b97-b2c2-13f7cbfc1688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am considering applying for this position, but I am unsure of the process, could you provide some guidance?"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Rewrite the following sentence in better English:\n",
    "I think I want to apply for this position but don't know how, can you help?\n",
    "\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77da66fb-3eb8-4fe6-8744-9c90957f4088",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am considering applying for this position but I am unsure of the process, could you provide some guidance?"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Rewrite the following sentence in better English:\n",
    "I think I want to apply for this position but don't know how, can you help?\n",
    "\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71e5cb14-1bbe-4ff3-a570-f2cb25742d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "This error occurs because the `zip` module is not installed on your system. The `zip` module provides functionality for working with archives and packing/unpacking files. It is a built-in module in Python, so it should be available by default.\n",
      "\n",
      "To fix this error, you can try installing the `zipfile` package using pip:\n",
      "```\n",
      "pip install zipfile\n",
      "```\n",
      "If you are running this code on a Jupyter Notebook, you can also try installing the `zipfile` package using the following command in your notebook cell:\n",
      "```\n",
      "!pip install zipfile\n",
      "```\n",
      "After installing the `zipfile` package"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"I am getting the following error when attempting to run this pythong code, can you explain why?\n",
    "  File \"/home/studio-lab-user/sagemaker-studiolab-notebooks/ha.py\", line 2, in <module>\n",
    "    import zip\n",
    "ModuleNotFoundError: No module named 'zip'\n",
    "\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f84be33-49d8-463a-99ae-44b099b28da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduced paragraph:\n",
      "Jeff Bezos lives at ### Main St., ###, FL ###. His phone number is ###-###-####."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Reduct PII from the following paragraph. Replace any PII with ###.\n",
    "Paragraph:\n",
    "Jeff Bezos lives at 1 Main St. Miami, FL 39812. His phone number is 111-123-4567.\n",
    "\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a806d-7351-408c-b141-fee35b1853b7",
   "metadata": {},
   "source": [
    "---\n",
    "The last one or two examples didn't quite work. Perhaps the model is not strong enough for this type of task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b86c6b2f-da12-4fef-b1e7-3597610d4e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: C) Photosynthesis is the process by which plants use sunlight, carbon dioxide, and water to produce glucose, oxygen, and other chemical compounds."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Create multiple choice question to test student's understanding of photosynthesis. The question must have at least three distractors. Indicate the correct answer.\n",
    "\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02269b-5c43-4e13-8722-c5284a9ab978",
   "metadata": {},
   "source": [
    "---\n",
    "The above didn't work at all. Let's try with better prompt formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31117af-933d-4152-81a1-93572f209a52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are five multiple-choice questions on photosynthesis:\n",
      "\n",
      "1. What is the process by which plants convert light energy into chemical energy?\n",
      "a) Respiration\n",
      "b) Photosynthesis\n",
      "c) Cell division\n",
      "d) Fermentation\n",
      "\n",
      "2. Which of the following substances is not a product of photosynthesis?\n",
      "a) Oxygen\n",
      "b) Carbon dioxide\n",
      "c) Water\n",
      "d) Nitrogen\n",
      "\n",
      "3. What is the name of the pigment responsible for absorbing light energy during photosynthesis?\n",
      "a) Chlorophyll\n",
      "b) Hemoglobin\n",
      "c) Melanin\n",
      "d) Myoglobin\n",
      "\n",
      "4. Which of the following statements about photosynthesis is true?\n",
      "a) It occurs in the presence of oxygen.\n",
      "b) It requires light energy to occur.\n",
      "c) It produces carbon dioxide as a"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"<|im_start|>system\n",
    "You are an AI assistant which creates assessments to help educators evaluate students knowledge<|im_end|>\n",
    "<|im_start|>user\n",
    "Create multiple choice question to test student's understanding of photosynthesis. The question must have at least three distractors. Indicate the correct answer.<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac46a54-7a46-4e0d-b089-553106c1b25f",
   "metadata": {},
   "source": [
    "## Next steps: Want to use a different model with this notebook?\n",
    "\n",
    "Hugging Face have many models that you can use and drop in to code like this. We encourage you to play with the prompts above and other models. You may need to make modifications to the code above depending on the model you choose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5405c238-40b5-4bf0-8d2d-271f7465080d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
