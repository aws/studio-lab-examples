{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6602da5-3c8d-40dd-9211-a0009357d69a",
   "metadata": {},
   "source": [
    "# Experimenting Prompt Engineering - Chatbot\n",
    "\n",
    "In this notebook we will explore common prompt engineering techniques to get the most out of Large Language Models (LLMs). \n",
    "\n",
    "First you will experiment by loading a 7 billion parameter LLM within the notebook environment and throwing some prompts its way to see how differt prompt strategies can impact the results of the model. \n",
    "\n",
    "After trying different prompt engineering techniques, you will learn how to run a simple chatbot using what you learned. \n",
    "\n",
    "Finally, we'll review some pointers for how to put these concepts into practice by building on the sample code and swapping in different LLMs to create your own solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091fdae7-bb30-4e60-a2ec-01eaf20657e2",
   "metadata": {},
   "source": [
    "### Working Environment \n",
    "\n",
    "[![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/build-on-aws/generative-ai-prompt-engineering/blob/main/prompt-engineering-chatbot/prompt-engineering-chatbot.ipynb)\n",
    "\n",
    "\n",
    "This notebook has been designed, written and tested to run for free on [Amazon SageMaker Studio Lab](https://studiolab.sagemaker.aws/) with CPU.  Studio Lab is a free machine learning (ML) development environment that provides compute and storage (up to 15GB) at no cost with NO credit card required.\n",
    "\n",
    "You can sign up for Amazon SageMaker Studio Lab here: [https://studiolab.sagemaker.aws/]\n",
    "\n",
    "> Whatever environment you end up using, make sure you have at least 12 GB of disk space available to run this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8a19a-8522-4bcf-b2a5-54cfcad0aead",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "First, if needed, install `ctransformers` - a library based on `transformers` from [Hugging Face](https://huggingface.co/), a great open source set of libraries for working and experimenting with the underlying technology of generative AI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa1ac45-da8a-441e-83d6-65e8132bba2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install ctransformers>=0.2.24 --quiet\n",
    "pip install langchain --quiet\n",
    "pip install transformers[torch] --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dc104e-d8e9-489b-ab21-dbcecbc3ecca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04e3ec-9978-45f5-9a00-e41ba50804f4",
   "metadata": {},
   "source": [
    "## Mistral-7B-Instruct-v0.1-GGUP Model\n",
    "\n",
    "The üêã [Mistral-7B-Instruct-v0.1-GGUP](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF) üêã model was fine-tuned on top of Mistral 7B using using a variety of publicly available conversation datasets. \n",
    "\n",
    "### Loading the model\n",
    "\n",
    "The following cell will load the pretrained model. Because the model itself is around 3.8 GB, so it can take a minute or so to load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d237a5b-0ee3-4b48-a426-4048b186a375",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b38b6b1b16b419db7b0865596c0945d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deac952125ce4fa3b2ca2e770fea2b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", model_type=\"mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a31903-630d-4b12-bea0-a77a872b61ac",
   "metadata": {},
   "source": [
    "# How do large language models work?\n",
    "\n",
    "## Prompt engineering\n",
    "\n",
    "Remember that LLMs are trained to predict the next word when given a sequence of words. \n",
    "\n",
    "![LLM-concept](img/LLM-generation.png)\n",
    "\n",
    "The way that we prompt an LLM can make a big difference in how it responds and how useful its output is. This depends not only on they type of query, but also nuances of how individual models where trained. Let's start by asking the Mistral model to describe the concept of photosynthesis.\n",
    "\n",
    "## Let's try a basic prompt\n",
    "\n",
    "In the following cell, we provide the model three parameters:\n",
    "1. the prompt: \"Explain photosythesis\"\n",
    "2. max_new_tokens which is the maximum length in tokens for the model's response\n",
    "3. temperature which you can think of as a measure of how *creative* the model can be in its response\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note:</b> If you are running this on a cpu instance of Studio Lab keep in mind that the following cell may take a couple minutes to run.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d5a91d1-2198-4583-90b0-8841885b3e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, water, and carbon dioxide into glucose (sugar), oxygen, and other chemical compounds. It is an essential process for life on Earth because it provides the primary source of organic compounds and oxygen.\n",
      "\n",
      "The process of photosynthesis occurs in two stages: the light-dependent reactions and the light-independent reactions. During the light-dependent reactions, light energy is absorbed by pigments in the chloroplasts, primarily chlorophyll. This energy is used to generate ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate), which are high-energy molecules that are used in the second stage of photosynthesis.\n",
      "\n",
      "During the light-independent reactions, also known as the Calvin cycle, ATP and NADPH are used to power a series of chemical reactions that convert carbon dioxide into glucose. This process is also known as carbon fixation. The glucose produced during photosynthesis is"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain photosythesis\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79335c8f-ede7-4a65-813e-ec225822d5db",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model did pretty well. Notice it started with generating a period (\".\"). That's because LLMs are trained to generate next most probably token, given an input. Let's see if we can make the answer more concise, and we will end our prompt with a period this time.</b></div>\n",
    "\n",
    "## Modifying the basic prompt\n",
    "Below we will try giving the model more specific instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18fe60fe-bc65-4855-819e-a7ecb3071e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, water, and carbon dioxide into glucose, oxygen, and other chemical compounds. This process occurs in the chloroplasts of plant cells and involves two stages: the light-dependent reactions, which use light energy to produce ATP and NADPH, and the light-independent reactions, also known as the Calvin cycle, which use these energy-rich molecules to fix carbon dioxide into glucose. Photosynthesis is a vital process for life on Earth, providing the primary source of food and oxygen for nearly all living organisms."
     ]
    }
   ],
   "source": [
    "prompt = \"Explain photosythesis in three sentences.\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7071987-b179-4cd2-900c-28e02894c7d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>This response is more concise, our prompt worked quite well.</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc398e-65a6-4ed6-99bc-19a3f2fa0498",
   "metadata": {},
   "source": [
    "# Fine-Tune with prompt\n",
    "\n",
    "When interact with large lanauge models, you can enhance the model performance by Weight fine-tuning, and instruction fine-tuning.\n",
    "\n",
    "- Weight fine-tuning involves taking a pre-trained language model and continuing to train it on a downstream task, tweaking the weights of the model through additional training on task-specific data. This allows the model to become specialized for a particular application. One technique of weight fine-tuning is Low-Rank Adaptation(LoRA). Read more about it: [A Brief Introduction to LoRA: Low-Rank Adaptation of Large Language Models](https://wandb.ai/sauravmaheshkar/LoRA/reports/A-Brief-Introduction-to-LoRA-Low-Rank-Adaptation-of-Large-Language-Models--Vmlldzo2MDAyOTU3)\n",
    "\n",
    "- Instruction fine-tuning keeps the weights of the pre-trained model fixed, but provides the model with natural language instructions that guide its behavior on a downstream task. For example, you could provide a prompt like \"Act as a helpful assistant that answers questions clearly and honestly.\" This prompts the model to adapt its behavior without changing the underlying weights. You can learn more about instruction fine-tuning in [How to Fine-Tune an LLM Part 1: Preparing a Dataset for Instruction Tuning](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2)\n",
    "\n",
    "Weight fine-tuning will provide more fitting into the prepared dataset, while it takes much more effort, time, and compute resources to achieve it. In this notebook, we start from the instruction fine-tuning.\n",
    "\n",
    "# Instruction fine-tuning\n",
    "\n",
    "So far we've been prompting the model to complete our inputs in a way so that we'd get the answers we wanted. Luckily, LLMs can be instruction tuned, meaning we could instruct the model to do specific tasks. This particular model was instruction tuned as described in the [model card](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF). It appears the model uses the following markup language:\n",
    "```\n",
    "<s>[INST] {prompt} [/INST]\n",
    "```\n",
    "\n",
    "Here is an example prompt exchange using the model template:\n",
    "\n",
    "```\n",
    "<s>[INST] What is your favourite condiment? [/INST]\n",
    "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>\n",
    "[INST] Do you have mayonnaise recipes? [/INST]\n",
    "```\n",
    "\n",
    "## Let's try it:\n",
    "\n",
    "The following cell uses the prompt template to provide a system prompt to the model to ask it to provide its reasoning step-by-step which can help LLM response accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8390b3-2d2b-4bfb-ab8c-223dc5455321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Please tell me about how mistral winds have attracted super-orcas. Keep your answer brief. [/INST]\n",
      "\n",
      "Response:\n",
      "Mistral winds are strong, cold winds that blow across the Mediterranean Sea from France to Spain. These winds are known for their strength and direction, which can reach up to 100 km/h and come from a specific direction. Super-orcas, also known as killer whales, have been attracted to these mistral winds due to their ability to generate large waves that can be used for hunting prey. The strong currents and waves created by the mistral winds provide an ideal environment for super-orcas to hunt and rest, making them a popular destination for these marine mammals."
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"\n",
    "<s>[INST] Please tell me about how mistral winds have attracted super-orcas. Keep your answer brief. [/INST]\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=500, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b7a08-1c97-47ef-a659-7cf30315567d",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model was able to follow the system direction to provide its step-by-step reasoning. However, manually supplying the full prompt template for each query is cumberson. In the next section we will look at how to simplify and automate prompt generation dynamically.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046f28b-a89f-41f5-9d54-89263a396039",
   "metadata": {},
   "source": [
    "# Dyanamic prompting\n",
    "Now that we know how to submit instructions to Mistral LLM, let's try assembling prompts dynamically - part of the prompt can be fixed, and part can be provided on the fly. We achieve this by creating prompt template with variables. The value for each variable is supplied in a separate statement and that statement can be executed further down in the code. Basically, this is a way to de-couple static part of the prompt from variable one, making the entire prompt dynamic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa310f2-356c-47aa-9679-cb7cba07ed48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "<s>[INST] {question} [/INST]    \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b97e69-cfaf-4d51-973c-797629f2c86b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Explain photosynthesis in three sentences [/INST]    \n",
      "\n",
      "Response:\n",
      "Photosynthesis is the process by which plants, algae and some bacteria convert sunlight, water and carbon dioxide into glucose, oxygen and other chemical compounds. It is an essential process for the survival of most life forms on earth as it provides the primary source of energy and food. The process occurs in two stages; the light-dependent reactions which take place in the thylakoid membranes of chloroplasts, and the light-independent reactions which occur in the stroma of the same organelles."
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(question=\"Explain photosynthesis in three sentences\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=500, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8c068-56e8-46a1-b2e2-f4c01e115c95",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>Using prompt templates makes it much easier to automate away the formatting nuances of different LLMs which can vary widely. Also notice, we did not finish our sentence with a period and the model did not try to complete it, becuase this time we signaled it that this is a command, an instrution which needs to be followed.</b></div>\n",
    "\n",
    "\n",
    "# Few-shot prompts\n",
    "\n",
    "We can influence model's output style by using what's called few-shot prompting - a technique which shows the model the exact behavior we expect on few examples. Few-shot prompting can be a powerful approach to improve model performance for a given task without additional training or fine-tuning. \n",
    "\n",
    "As you can see in the cell below, we provide the model two examples of a prompt and the expected response where the number of response sentences is constrained correctly by the request. We also influence the output formatting by demonstrating that we prefer sentences to be number in each response. \n",
    "\n",
    "In the end the full prompt with our request for a three sentence explanation of photosenthesis and let the model complete the example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e47308-970e-4211-9923-26eed91d2809",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Photosynthesis is the process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy in the form of glucose or other sugars.\n",
      "2. This process occurs in the chloroplasts of plant cells, where chlorophyll and other pigments absorb light photons and use them to power a series of chemical reactions that split water molecules into hydrogen and oxygen.\n",
      "3. The oxygen produced during photosynthesis is released into the atmosphere as a byproduct, while the glucose can be used by the plant for energy or stored for later use."
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"\n",
    "<s>[INST] Explain precipitation in two sentences [/INST]\n",
    "1. In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.\n",
    "2. The main forms of precipitation include drizzle, rain, sleet, snow, ice pellets, graupel and hail.</s>\n",
    "[INST] Explain condensation in one sentence [/INST]\n",
    "1. Condensation is the change of the state of matter from the gas phase into the liquid phase, and is the reverse of vaporization.</s>\n",
    "[INST] Explain photosynthesis in three sentences  [/INST]\n",
    "\"\"\"\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02938f76-69c4-408c-9a59-7200293814e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>And now with langchain template:</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49a23538-bed3-46c5-9343-16aaf9a86f72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<s>[INST] Explain precipitation in two sentences. [/INST]\n",
      "1. In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.\n",
      "2. The main forms of precipitation include drizzle, rain, sleet, snow, ice pellets, graupel and hail.</s>\n",
      "\n",
      "<s>[INST] Explain condensation in one sentence. [/INST]\n",
      "1. Condensation is the change of the state of matter from the gas phase into the liquid phase, and is the reverse of vaporization.</s>\n",
      "\n",
      "<s>[INST]Explain photosynthesis in three sentences.[/INST]\n",
      "Response:\n",
      " 1. Photosynthesis is the process by which plants use sunlight, carbon dioxide, and water to produce oxygen and glucose.\n",
      "2. It occurs in the chloroplasts of plant cells, where chlorophyll captures light energy and uses it to convert chemical energy from water molecules into glucose.\n",
      "3. The byproduct of photosynthesis is oxygen, which is released into the atmosphere during the process."
     ]
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "fs_template = PromptTemplate.from_template(\n",
    " \"\"\"<s>[INST] {question} [/INST]\n",
    "{output}</s>\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"Explain precipitation in two sentences.\",\n",
    "    \"output\": \"1. In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.\\n2. The main forms of precipitation include drizzle, rain, sleet, snow, ice pellets, graupel and hail.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Explain condensation in one sentence.\",\n",
    "    \"output\": \"1. Condensation is the change of the state of matter from the gas phase into the liquid phase, and is the reverse of vaporization.\"\n",
    "  }\n",
    "]\n",
    "\n",
    "prompt_fs = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=fs_template,\n",
    "    suffix=\"<s>[INST]{question}[/INST]\",\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_fs.format(question=\"Explain photosynthesis in three sentences.\")\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "      \n",
    "print(\"Response:\")      \n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4fb9a-cb8a-4940-9715-4d70f4c215b0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Using a couple examples, we were able to teach the model to respond by specifically numbering the sentences in its response and limiting its response to the number of sentences requested.</b></div>\n",
    "\n",
    "\n",
    "# Augmenting LLM response with context\n",
    "\n",
    "Without providing any real-time context, the model is limited to knowledge it gained during training which may be incomplete or out of date. However, we can improve responses by providing relevant context that the model can use before formulating a response. \n",
    "\n",
    "To illustrate the benefit of providing context we will first query the model about which instance types are available to use for managed spot training in SageMaker. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f102be3d-1d7f-43c9-aa79-dbc2ccd21753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Which instances can I use with Managed Spot Training in SageMaker? [/INST]    \n",
      "\n",
      "Response:\n",
      "Managed Spot Training is a feature of Amazon SageMaker that allows you to train machine learning models using Amazon EC2 Spot Instances. You can use Managed Spot Training for a wide range of scenarios, including:\n",
      "\n",
      "1. Image and video classification: You can use Managed Spot Training to train models for image and video classification tasks, such as object detection, facial recognition, and activity recognition.\n",
      "2. Natural language processing (NLP): You can use Managed Spot Training to train NLP models for tasks such as text classification, sentiment analysis, and named entity recognition.\n",
      "3. Predictive analytics: You can use Managed Spot Training to train predictive models for a variety of applications, including fraud detection, customer churn prediction, and demand forecasting.\n",
      "4. Deep learning: You can use Managed Spot Training to train deep learning models for tasks such as image recognition, speech recognition, and natural language processing.\n",
      "5. Reinforcement learning: You can use Managed Spot Training to train reinforcement learning models for applications such as game playing, robotics, and autonomous driving.\n",
      "\n",
      "Overall, Managed Spot Training is a versatile tool that can be used for a wide range of machine learning tasks."
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(question=\"Which instances can I use with Managed Spot Training in SageMaker?\")\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")      \n",
    "for text in llm(prompt, max_new_tokens=300, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b4409-3c5e-4c23-8a3b-39897bfee683",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>This response does not clearly and directly answer our question and you can see how without provided context, the LLM uses its own \"knowledge\" to answer questions. Let's try providing some context to the model to use as reference. We will add a new dynamic field \"context\" to our prompt template and provide up to date information when we ask the user question.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6110e2e4-773f-4ff6-a0d3-3d33073cae42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Given the context below, answer the question that follows. If you do not know the answer and the context doesn't contain the answer truthfully say \"I don't know\".\n",
      "\n",
      "Context: Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n",
      "Question: Which instances can I use with Managed Spot Training in SageMaker?[/INST]\n",
      "\n",
      "Response:\n",
      "You can use all instances supported in Amazon SageMaker with Managed Spot Training."
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "context_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "<s>[INST] Given the context below, answer the question that follows. If you do not know the answer and the context doesn't contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}[/INST]\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "context = \"\"\"Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\"\"\"\n",
    "\n",
    "\n",
    "prompt=context_template.format(context=context, question=\"Which instances can I use with Managed Spot Training in SageMaker?\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31be60b-e4e5-4e3c-98e2-a8ef42ceb889",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>Great! The model used the provided context to answer the user question. Now lets use the same context and ask the model about something that is not covered in the context. </b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f59ac84e-dd06-46c2-88ef-d5f0c9547f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Given the context below, answer the question that follows. If you do not know the answer and the context doesn't contain the answer truthfully say \"I don't know\".\n",
      "\n",
      "Context: Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n",
      "Question: Which instances can I use with Amazon ECS?[/INST]\n",
      "\n",
      "Response:\n",
      "I don't know, the context does not mention anything about using Managed Spot Training with Amazon ECS."
     ]
    }
   ],
   "source": [
    "prompt=context_template.format(context=context, question=\"Which instances can I use with Amazon ECS?\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4324af-bfa3-4c47-a43b-ee6393fad0c5",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model followed our direction and admitted that it did not know the answer when it wasn't able to be determined from the provided context. This is a powerful method that is used heavily in RAG (Retrieval Augmented Generation) applications. A company may only want an IT support bot to provide answers based on internal documentaiton provided to the model as context. </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd469ce-4e37-4cec-8168-cadd5d072237",
   "metadata": {},
   "source": [
    "# Hallucinations\n",
    "\n",
    "Hallucination is when a model makes up information and presents it as if it were true. Leading questions can often cause hallucinations. Let's see what happens if we ask about a non-existent new launch at re:Invent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51c3685e-03e7-4072-bc8c-8a67b63aaa7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] What type of new chemical solution AWS announced at re:Invent this year? [/INST]    \n",
      "\n",
      "Response:\n",
      "At re:Invent 2021, AWS announced a new chemical solution called \"AWS Elemental AI for Chemical Processing.\" This solution is designed to help chemical processors optimize their operations by using machine learning and artificial intelligence (AI) techniques. It can be used to predict equipment failures, improve yield and throughput, reduce energy consumption, and enhance product quality. The solution leverages AWS's existing services such as Amazon SageMaker, Amazon Redshift, and Amazon Kinesis Data Streams to provide a comprehensive platform for chemical processors."
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(question=\"What type of new chemical solution AWS announced at re:Invent this year?\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa8272-6572-47da-a365-b23ac32be304",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model incorrectly states that AWS announced a new chemical solution called \"AWS Graviton2\" in 2019 (you may get a different answer). One approach to combatting LLM hallucination is to give the model an \"out\" by giving it explicit instructions not to make up information and admit when it doesn't know something. Let's try the same question again by by also adding additional instructions in our prompt as follows: </b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b931cf35-e59e-491c-8772-0cda7f021590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] What type of new chemical solution AWS announced at re:Invent this year? Do not make up facts. Say I don't know if you have no information about something or unsure. [/INST]    \n",
      "\n",
      "Response:\n",
      "I apologize, but I do not have any specific information on the new chemical solution that AWS announced at re:Invent this year. Can you please provide me with more context or details so that I can assist you better?"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(question=\"What type of new chemical solution AWS announced at re:Invent this year? Do not make up facts. Say I don't know if you have no information about something or unsure.\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f21e2-4b5f-4313-86e5-3885b2f10f1d",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>By giving the model clear instructions and an out, we were able to get the model to admit it didn't have enough context to answer the question rather than making up something.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fb206-6661-4ba6-95c5-6369176ff869",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "\n",
    "Let's make a simple chatbot.  There is no special library to include and no setting to apply to the LLM, all we need is prompt engineering!\n",
    "\n",
    "Let's use what we know of prompts with in context learning, to create a simple chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "898d619d-6c08-46d4-a0ba-cb1a9e23a7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Who is Jeff Bezos? [/INST]    \n",
      "\n",
      "Response:\n",
      "Jeff Bezos is an American entrepreneur and business magnate, born on July 12, 1964 in Seattle, Washington. He is best known as the founder of Amazon, a multinational technology company focusing on e-commerce, cloud computing, digital streaming, and artificial intelligence. Bezos has also ventured into space exploration with his company Blue Origin. He has been ranked as one of the world's wealthiest people by Forbes and Time magazine.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is Jeff Bezos?\"\n",
    "\n",
    "prompt = prompt_template.format(question=question)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "resp = llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75da0c-cce2-4f04-8cc0-65ea5b1efc5b",
   "metadata": {},
   "source": [
    "### Follow-up questions\n",
    "Now if we ask the LLM a follow-up question without providing context from the conversation so far, it will not be able to provide an accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae5b02ab-f7d3-4022-b26d-85284e8fe9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] How old is he? [/INST]    \n",
      "\n",
      "Response:\n",
      "I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. Therefore, I don't know how old he is. Can you please provide more context or information about who you are referring to?"
     ]
    }
   ],
   "source": [
    "question = \"How old is he?\"\n",
    "\n",
    "prompt = prompt_template.format(question=question)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=300, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9422bf01-56ab-4d94-8395-bc8ffb592dd6",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model doesn't know who \"he\" is. Without the chat history context it is not able to answer.</b></div>\n",
    "\n",
    "### Providing chat context\n",
    "Let's use the context template we created earlier and feed in the previous response about Jeff Bezos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4500db7-a2da-40ce-b525-a33eacb46c72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Given the context below, answer the question that follows. If you do not know the answer and the context doesn't contain the answer truthfully say \"I don't know\".\n",
      "\n",
      "Context: Jeff Bezos is an American entrepreneur and business magnate, born on July 12, 1964 in Seattle, Washington. He is best known as the founder of Amazon, a multinational technology company focusing on e-commerce, cloud computing, digital streaming, and artificial intelligence. Bezos has also ventured into space exploration with his company Blue Origin. He has been ranked as one of the world's wealthiest people by Forbes and Time magazine.\n",
      "Question: How old is he?[/INST]\n",
      "\n",
      "Response:\n",
      "Jeff Bezos was born on July 12, 1964. To calculate his current age, subtract his birth year from the current year (2021), and you get 57 years old."
     ]
    }
   ],
   "source": [
    "prompt = context_template.format(context=resp, question=question)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=300, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84aba05-9ae9-4511-a506-8bbf558854e9",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>This is a simplified example, but by including the past chat context, we can support a back and forth chat bot interaction with the LLM.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd190db-6d79-4a00-9d67-b0bfb4019a3f",
   "metadata": {},
   "source": [
    "# More examples of LLM use cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7671b-87da-453b-a400-6f125d644b38",
   "metadata": {},
   "source": [
    "## The following are optional prompts you can try running to keep exploring.\n",
    "\n",
    "\n",
    "For simpler questions/prompts we can often get away witout strict instruction formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c507742b-dab9-4b97-b2c2-13f7cbfc1688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am considering applying for this position, however, I am uncertain about the process and would appreciate your assistance."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Rewrite the following sentence in better English:\n",
    "I think I want to apply for this position but don't know how, can you help?\n",
    "\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71e5cb14-1bbe-4ff3-a570-f2cb25742d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "This error occurs when the Python interpreter is unable to find the `zip` module, which is a built-in module in Python that provides functionality for working with archives. The `zip` module is required by the code you provided, so it needs to be imported in order for the code to run.\n",
      "\n",
      "To fix this error, you can try the following steps:\n",
      "\n",
      "1. Make sure that you are running the code on a system that has Python installed. If you don't have Python installed, you can download and install it from the official website (<https://www.python.org/downloads/>).\n",
      "2. Check that the version of Python you are using is compatible with the version"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"I am getting the following error when attempting to run this pythong code, can you explain why?\n",
    "  File \"/home/studio-lab-user/sagemaker-studiolab-notebooks/ha.py\", line 2, in <module>\n",
    "    import zip\n",
    "ModuleNotFoundError: No module named 'zip'\n",
    "\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f84be33-49d8-463a-99ae-44b099b28da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduced Paragraph:\n",
      "Jeff Bezos lives at ### St. ###, ### ### ###. His phone number is ###-###-####."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Reduct PII from the following paragraph. Replace any PII with ###.\n",
    "Paragraph:\n",
    "Jeff Bezos lives at 1 Main St. Miami, FL 39812. His phone number is 111-123-4567.\n",
    "\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a806d-7351-408c-b141-fee35b1853b7",
   "metadata": {},
   "source": [
    "---\n",
    "The last one or two examples didn't quite work. Perhaps the model is not strong enough for this type of task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac46a54-7a46-4e0d-b089-553106c1b25f",
   "metadata": {},
   "source": [
    "## Next steps: Want to use a different model with this notebook?\n",
    "\n",
    "Hugging Face have many models that you can use and drop in to code like this. We encourage you to play with the prompts above and other models. You may need to make modifications to the code above depending on the model you choose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5405c238-40b5-4bf0-8d2d-271f7465080d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
